{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQxCmM5qHBC7"
      },
      "source": [
        "## **Without Standardize Method**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CldM-yGGBxqg"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3DTlWyEAvwC"
      },
      "outputs": [],
      "source": [
        "# lets creat a llm component\n",
        "\n",
        "\n",
        "class MyLLM:\n",
        "    def __init__(self):\n",
        "        print(\"LLM created\")\n",
        "\n",
        "    # our llm will get a prompt and based on the prompt our llm will response\n",
        "    def predict(self, prompt):\n",
        "        # create random list and for every prompt\n",
        "\n",
        "        response = [\n",
        "            \"Delhi is the capital of India\",\n",
        "            \"IPL is a cricket league\",\n",
        "            \"AI stands for Artificial Intelligence\",\n",
        "        ]\n",
        "\n",
        "        # randomly choose any response\n",
        "        return {\"response\": random.choice(response)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf0ARNAgCJWg",
        "outputId": "1372e599-d8db-4478-dfda-9b84adfcf6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM created\n"
          ]
        }
      ],
      "source": [
        "llm = MyLLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eii8FVrRCMIi",
        "outputId": "57b41ce7-4619-41a0-eff0-d4a7ef1b3d4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'AI stands for Artificial Intelligence'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.predict(\"what is ai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ_v8BLECPhN"
      },
      "outputs": [],
      "source": [
        "class myPromptTemplate:\n",
        "    def __init__(self, template, input_variable):\n",
        "        self.template = template\n",
        "        self.input_variable = input_variable\n",
        "\n",
        "    # prompts has a format method\n",
        "    def myformat(self, input_dict):\n",
        "        # we are calling this function within itself with template to have this input dict and using start start for no limit\n",
        "        return self.template.format(**input_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Pv5HvsDszV"
      },
      "outputs": [],
      "source": [
        "template = myPromptTemplate(\n",
        "    template=\"Write a song about {topic}\", input_variable=[\"topic\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zSUOdDEzD2Ri",
        "outputId": "b820a90c-3994-4b6f-c579-83d9a70da914"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Write a song about India'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template.myformat({\"topic\": \"India\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ7_w-_ZD_7y"
      },
      "outputs": [],
      "source": [
        "# we can pass multiple input variables\n",
        "template = myPromptTemplate(\n",
        "    template=\"Write a {length} song about {topic}\", input_variable=[\"topic\", \"length\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PVaUiBdvEl3q",
        "outputId": "e8a37711-c5e1-434f-9062-e368d0036fa4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Write a short song about India'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template.myformat({\"topic\": \"India\", \"length\": \"short\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPpt6lStEu7k"
      },
      "source": [
        "Now we need to make a llm application based on both these class, first create llm and make prompt and make\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snh4WioKErBR",
        "outputId": "c17340be-ea30-4c47-9a8f-c652c67f4c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM created\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'response': 'IPL is a cricket league'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample Components connecting\n",
        "\n",
        "# llm create\n",
        "llm = MyLLM()\n",
        "\n",
        "prompt = myPromptTemplate(\n",
        "    template=\"This is a sample for my Prompt Template for {type} llm\",\n",
        "    input_variable=[\"type\"],\n",
        ")\n",
        "\n",
        "prompt = prompt.myformat({\"type\": \"custom\"})\n",
        "\n",
        "llm.predict(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS4eJ7zhFsUX"
      },
      "source": [
        "Now we need to Chain those components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKslAVMrFcEK"
      },
      "outputs": [],
      "source": [
        "class MyLLMChain:\n",
        "    def __init__(self, llm, prompt):\n",
        "        self.llm = llm\n",
        "        self.prompt = prompt\n",
        "\n",
        "    def my_chain_run(self, input_dict):\n",
        "        final_prompt = self.prompt.myformat(input_dict)\n",
        "        result = self.llm.predict(final_prompt)\n",
        "        # extract string from the response\n",
        "        return result[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkInm5T5GT-S"
      },
      "outputs": [],
      "source": [
        "# prompt\n",
        "prompt = myPromptTemplate(\n",
        "    template=\"This is a sample for my Prompt Template for {type} llm\",\n",
        "    input_variable=[\"type\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTOSP_QMGYbC",
        "outputId": "67e27c4e-1953-4bcd-fa1f-3bbe72c64f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM created\n"
          ]
        }
      ],
      "source": [
        "llm = MyLLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9umRDW9Gabz"
      },
      "outputs": [],
      "source": [
        "chain = MyLLMChain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g6D3Av9xGgkJ",
        "outputId": "b02985c6-6280-47cd-8340-45dbaade242e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'AI stands for Artificial Intelligence'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.my_chain_run({\"type\": \"custom\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQXC0cGEGqBL"
      },
      "source": [
        "This is how the Team of Langchain thought to create chains We can see we only need to pass the input at one time and by only calling one function we get the output for both\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abhVLWwrHHWK"
      },
      "source": [
        "## **With Standardize Method**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay9JA99lHcVC"
      },
      "source": [
        "We need to standardize both the class to make flexible chains\n",
        "\n",
        "1.  Make components Standardize\n",
        "2.  Chain those components\n",
        "\n",
        "---\n",
        "\n",
        "How we are going to do so.\n",
        "\n",
        "- Convert all components into `runnable`\n",
        "- All components have same methods --> this is Solid Application of Abstraction so that each component have same methods which we can make sure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5-10f0M6IU2G"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4VGzLlDIYHC"
      },
      "outputs": [],
      "source": [
        "# Create runnable as Abstract class\n",
        "class Runnable(ABC):\n",
        "    @abstractmethod\n",
        "    def invoke(input_data):\n",
        "        pass\n",
        "        # This will make sure that each class that inherits from Runnable has invoke method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg3drzIEGoUL"
      },
      "outputs": [],
      "source": [
        "class MyLLM(Runnable):\n",
        "    def __init__(self):\n",
        "        print(\"LLM created\")\n",
        "\n",
        "    def predict(self, prompt):\n",
        "        response = [\n",
        "            \"Delhi is the capital of India\",\n",
        "            \"IPL is a cricket league\",\n",
        "            \"AI stands for Artificial Intelligence\",\n",
        "        ]\n",
        "\n",
        "        return {\"response\": random.choice(response)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "-B5rHgrcIsHz",
        "outputId": "b424081e-3790-4761-a64d-1da4373e8237"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Can't instantiate abstract class MyLLM without an implementation for abstract method 'invoke'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3539893831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class MyLLM without an implementation for abstract method 'invoke'"
          ]
        }
      ],
      "source": [
        "llm = MyLLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYZbdlMnIwa1"
      },
      "outputs": [],
      "source": [
        "class MyLLM(Runnable):\n",
        "    def __init__(self):\n",
        "        print(\"LLM created\")\n",
        "\n",
        "    def predict(self, prompt):\n",
        "        response = [\n",
        "            \"Delhi is the capital of India\",\n",
        "            \"IPL is a cricket league\",\n",
        "            \"AI stands for Artificial Intelligence\",\n",
        "        ]\n",
        "\n",
        "        print(\"This method is going to be depriciated in future with the invoke method\")\n",
        "        return {\"response\": random.choice(response)}\n",
        "\n",
        "    # This will work same but with its own implementation\n",
        "    def invoke(self, prompt):\n",
        "        response = [\n",
        "            \"Delhi is the capital of India\",\n",
        "            \"IPL is a cricket league\",\n",
        "            \"AI stands for Artificial Intelligence\",\n",
        "        ]\n",
        "\n",
        "        return {\"response\": random.choice(response)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM3rAqc9HOfi"
      },
      "outputs": [],
      "source": [
        "class myPromptTemplate(Runnable):\n",
        "    def __init__(self, template, input_variable):\n",
        "        self.template = template\n",
        "        self.input_variable = input_variable\n",
        "\n",
        "    def myformat(self, input_dict):\n",
        "        print(\"This method is going to be depriciated in future with invoke() method\")\n",
        "        return self.template.format(**input_dict)\n",
        "\n",
        "    def invoke(self, input_dict):\n",
        "        return self.template.format(**input_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsbUTnANHRHi"
      },
      "outputs": [],
      "source": [
        "class RunnableConnector(Runnable):\n",
        "    def __init__(self, runnable_list):\n",
        "        self.runnable_list = runnable_list\n",
        "\n",
        "    def invoke(self, input_data):\n",
        "        for runnable in self.runnable_list:\n",
        "            # runnable.invoke(input_data)\n",
        "            # we will store the output as input for next step\n",
        "            input_data = runnable.invoke(input_data)\n",
        "\n",
        "        # after loop ends\n",
        "        return input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3IeYPshIKoBp",
        "outputId": "9d0e62b5-6f71-4f32-985d-d9db2e1cf5f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Now we can run chains'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Now we can run chains\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmEDj1MHKq0I"
      },
      "outputs": [],
      "source": [
        "template = myPromptTemplate(\n",
        "    template=\"Write a {length} song about {topic}\", input_variable=[\"topic\", \"length\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ErbENp_KwO6",
        "outputId": "6f90777f-7f1e-4727-e6b5-f595363d6377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM created\n"
          ]
        }
      ],
      "source": [
        "llm = MyLLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ifdRt2aLKyex"
      },
      "outputs": [],
      "source": [
        "chain = RunnableConnector([template, llm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQZRHsq-K5uy",
        "outputId": "16b02f73-cb0e-4ee9-fdc3-deca89efaa0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'AI stands for Artificial Intelligence'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"India\", \"length\": \"Custom\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gOSbIxhLK3m"
      },
      "source": [
        "**Now main feature of this is we can pass any component through this chain and it will work similar**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnJVejHpLIuq"
      },
      "outputs": [],
      "source": [
        "class MyStringOutParser(Runnable):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def invoke(self, input_str):\n",
        "        return input_str[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jif8NURqLo7Z"
      },
      "outputs": [],
      "source": [
        "parser = MyStringOutParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "lLuaqby6Ls8K"
      },
      "outputs": [],
      "source": [
        "chain = RunnableConnector([template, llm, parser])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Tsf3TTHLvKJ",
        "outputId": "e88415cd-4958-4f67-e872-18f492364788"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'IPL is a cricket league'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"topic\": \"India\", \"length\": \"Custom\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x12Pbke5L3Yg"
      },
      "source": [
        "Now here we have seen that we have connected parser also with so ease. Just need to pass the parser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UXvgVSVMBDh"
      },
      "source": [
        "## **Now Connect Two Chains**\n",
        "\n",
        "1. 1st chain will be of making jokes\n",
        "2. 2nd chain will be of explaining jokes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zaxc3C4ZLxXh"
      },
      "outputs": [],
      "source": [
        "template1 = myPromptTemplate(\n",
        "    template=\"Write a joke about {topic}\", input_variable=[\"topic\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tmqtHomMWBi"
      },
      "outputs": [],
      "source": [
        "template2 = myPromptTemplate(\n",
        "    template=\"Explain the following joke {response}\", input_variable=[\"response\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en_2l5e9MbQJ",
        "outputId": "4796643c-9e74-4a96-9643-e37aa43ff367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM created\n"
          ]
        }
      ],
      "source": [
        "llm = MyLLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "y554-HKeMcyB"
      },
      "outputs": [],
      "source": [
        "parser = MyStringOutParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uwNVAZpMMehC"
      },
      "outputs": [],
      "source": [
        "# Chain 1\n",
        "chain1 = RunnableConnector([template1, llm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqxDt-XgMgTB",
        "outputId": "49201a68-ab3c-4139-b1ab-12fd604cd295"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': 'Delhi is the capital of India'}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain1.invoke({\"topic\": \"Cricket\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "EL5CZleFMo2h"
      },
      "outputs": [],
      "source": [
        "# Chain 2\n",
        "chain2 = RunnableConnector([template2, llm, parser])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4cWVi28LMzb5",
        "outputId": "28590fc3-4975-45b5-ebe6-ba7f5b9164ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Delhi is the capital of India'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain2.invoke({\"response\": \"Cricket\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bn0LCsxSM3uZ",
        "outputId": "56af39ae-4840-4bdb-e798-fd1e4f2a62de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Delhi is the capital of India'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now here is the magic where we can connect those two chains\n",
        "\n",
        "final_chain = RunnableConnector([chain1, chain2])\n",
        "\n",
        "final_chain.invoke({\"topic\": \"AI\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGQMmCN4NHWx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain-tutorials (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
